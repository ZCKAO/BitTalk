# -*- coding: utf-8 -*-
"""2021CC專案

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17S800Q6s_XzcIJzfjbdS2GtfUAdwRLLQ

詞對的社會網絡分析
* 詞對分析找出會共同出現的字詞。
* 字詞會共同出現一定有道理，我們姑且稱之為潛在語意，有個潛在語意在後面，把這些字詞放在一起。
* 語料庫是特定性質的字詞的集合，可以讓詞對分析更聚焦，更有目的性。
* 社會網絡分析則是希望以圖示的方式，更突顯潛在語意。
"""

# 語料庫必須是字詞，不是句子。

from collections import Counter
import pandas as pd
import numpy
import numpy as np
import matplotlib.pyplot as plt
import os
import io
import re

# 中文設定1: 楷體字
from matplotlib import rcParams
rcParams["font.sans-serif"] = ["kaiu"] 
rcParams["font.family"] ="DFKai-sb"

!gdown --id '1Z1fAwZCsfmG3eiSdlgk5mHqhXUNMF0nK' --output news.xlsx
!gdown --id '1dBWYhSLR469VkwhwWV-yjbXf3bewonk2' --output deletewords.txt
!gdown --id '1fcjlTgN_CXXxIPAysafhXz8f0tI-1jgZ' --output userdict.txt
!wget -N https://raw.githubusercontent.com/P4CSS/PSS/master/data/stopwords_zh-tw.txt
with open("stopwords_zh-tw.txt", encoding="utf-8") as fin:
    stopwords = fin.read().split("\n")[1:]

"""# 讀取資料"""

df = pd.read_excel('news.xlsx')
df.columns = ['Unnamed: 0', 'dates', 'title', 'address', 'content']
df

"""# 讀取停用字"""

file_name = "deletewords.txt"
deletewords = open(file_name,'r',encoding='utf-8').read()
deletewords=list(set(deletewords.split()))

df

"""# 資料清理
把不是文字的內文用空值取代
"""

content = []
for i,j in zip(df['content'], range(len(df['content']))):
  # print(i)
  if type(i)!=str or i=='':
    content.append('')
  else:
    content.append(i.replace('\n',''))
df['content'] = content

"""## 斷詞"""

import jieba
jieba.load_userdict('userdict.txt') #匯入自訂字典

df['content_token'] = df['content'].apply(lambda x:list(jieba.cut(x)))
df

"""## 清除標點符號

"""

import unicodedata # for removing Chinese puctuation
def remove_punc_by_unicode(words):
    out = []
    for word in words:
        if word != " " and not unicodedata.category(word[0]).startswith('P'):
            out.append(word)
    return out


df['cleaned_content_token'] = df['content_token'].apply(remove_punc_by_unicode)

df

"""# 字頻分析"""

restore = []
for i in df['cleaned_content_token']:
  for j in i:
    restore.append(j)

from collections import Counter
tf = Counter()
for sentence in restore:
  tf[sentence] += 1

filtered = dict(tf)

def removeAlphaNum(i): #去除英文與數字
  temp_ = []
  for sentence in i:
    if '\u4e00' <= sentence <= '\u9fa5':
      continue
    else:
      temp_.append(sentence)  
  for i in temp_:
    del filtered[i]
  
  return filtered



def removeChiStopWords(i):
  temp_ = []
  for sentence in i:
    if sentence in stopwords:
      temp_.append(sentence)
    else:
      continue  
  for i in temp_:
    del filtered[i]
  
  return filtered

filtered = removeChiStopWords(filtered)
filtered = removeAlphaNum(filtered)
print(filtered)

Counter(filtered).most_common(200)
[('美元', 29427),
 ('比特幣', 28627),
 ('加密', 26921),
 ('貨幣', 26283),
 ('交易', 20393),
 ('市場', 17092)
 ('區塊', 15455)

"""## 只用前後一個字來算共現詞"""

def removeAlphaNum(i): #去除英文與數字
  temp_ = []
  for sentence in i:
    if '\u4e00' <= sentence <= '\u9fa5':
      
      temp_.append(sentence)
    else:
      continue 
  return temp_



def removeChiStopWords(i):
  temp_ = []
  for sentence in i:
    if sentence in stopwords:
      continue  
    else:
      temp_.append(sentence)  
  return temp_

restore = removeAlphaNum(restore)
restore = removeChiStopWords(restore)

word_pair_counts = Counter()
for i in range(len(restore) - 1):
    (w1, w2) = (restore[i], restore[i + 1])
    word_pair_counts[(w1, w2)] += 1
    
for pair, c in word_pair_counts.most_common(500):
    print("%s\t%s\t%d" % (pair[0], pair[1], c))

"""# 自訂window size算共現詞"""

# window size !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
w=3

# 計算配對的次數
word_pair_count=Counter() 
word_counts = Counter(restore)
for i in range(len(restore)-1):
    # w=3
    for j in range(1,w+1):     
        if i+j < len(restore):            
            (w1,w2)=restore[i],restore[i+j]   
            word_pair_count[(w1,w2)]+=1
            num_bigrams += 1          
print ("total bigrams", num_bigrams)

word_pair_count

"""## 計算卡方值"""

# 計算卡方值
def chisquare(o11, o12, o21, o22):
    n = o11 + o12 + o21 + o22
    x_2 = (n * ((o11 * o22 - o12 * o21)**2)) / ((o11 + o12) * (o11 + o21) * (o12 + o22) * (o21 + o22)) 
    return x_2
pair_chi_squares = Counter()
for (w1, w2), w1_w2_count in word_pair_count.most_common():
        # O12
        w1_only_count = word_counts[w1] - w1_w2_count
        # O21
        w2_only_count = word_counts[w2] - w1_w2_count
        # O22
        rest_count = num_bigrams - w1_only_count - w2_only_count - w1_w2_count
        # O11= w1_w2_count
        pair_chi_squares[(w1, w2)] = chisquare(w1_w2_count, w1_only_count, w2_only_count, rest_count)

Counter(pair_chi_squares).most_common(200)

"""## edges, 篩選合於條件者排序

## 設定條件，結合語料庫
"""

kw=["比特幣", '狗狗幣']    # 可設多個比較 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! kw=["比特幣", '狗狗幣', ........]

corpus = restore

W1=[]
W2=[]
weight=[]
for (w1, w2), x_2 in pair_chi_squares.most_common():
    # 設定條件 ##############################################    
    if (len(w1)>1 and len(w2)>1) and (w1 in kw or w2 in kw):   
#      if (w1 in kw and w2 in corpus) or  (w2 in kw and w1 in corpus):   
     ########################################################    
        W1.append(w1)
        W2.append(w2)
        weight.append(x_2)
# edges 變數很重要，等一下社會網絡圖會用到    
edges=list(zip(W1, W2, weight))
edges

# 配對結果後複雜，可以再用語料庫減化

df=pd.DataFrame(edges,columns=["name1","name2","weight"])
df

"""# 詞對與社會網絡分析 
* 社會網絡分析
* 每個字詞為一個節點
* 與其有關鍵的字詞有連結
* 連結愈強，線條愈粗。
* networkX  如果無法出現中文字型，可參考以下做法： 
 * 目錄 C:\Users\\[USERNAME]\anaconda3\Lib\site-packages\matplotlib\mpl-data\fonts\ttf
 * 舊檔 DejaVuSans.ttf 改名為 DejaVuSans_bak.ttf
 * 拷貝新檔  DejaVuSans.ttf （請從教學網頁下載）

# 第一層最重要的關係取十個，不夠再加
"""

df1=df.iloc[:20]
df1

# 最後修正，刪除不要

df1.drop([18, 13], inplace=True)
df1.index=range(len(df1))
df1

"""## networkx
* 會有版本版本衝突問題，建議用 nodexl

# 第二回合的配對
"""

new=set(list(df1.name1)+list(df1.name2))
new.remove(kw[0])
new

"""## 挑選合於條件的配對"""

new=['以太'] # !!!!!!!!!!!!!!!!!!!!!!

second=df1[["name1","name2","weight"]].values.tolist()
for n in new:  ####
    W1=[]
    W2=[]
    weight=[]
    for (w1, w2), x_2 in pair_chi_squares.most_common():
        # 設定條件, 在 new 而且第一回合不存在 ########################################################
        if (len(w1)>1 and len(w2)>1) and ((w1 == n and w2 not in kw) or (w2 == n and w1 not in kw)):     
        ############################################################################################    
            W1.append(w1)
            W2.append(w2)
            weight.append(x_2)        
    edges2=list(zip(W1, W2, weight))
    dfs=pd.DataFrame(edges2,columns=["name1","name2","weight"])
    dfs=dfs.sort_values(by=['weight'],ascending=False)
    # 選取最大的十個
    dfs=dfs.iloc[:10]
    add=dfs.values.tolist()
    second+=add
df2=pd.DataFrame(second,columns=["name1","name2","weight"])

df2

"""## 重新繪圖"""

edges=df2.values.tolist()

# 各種不同的繪圖方法 ################################
# input: edges
G = nx.DiGraph()   # directed
# G = nx.Graph()     # undirected    
G.add_weighted_edges_from(edges)
edg =G.edges()
nod=G.nodes()

# 節點顏色大小
nod_size=1
nod_color="yellow"

# 連結寬度
wide=1
edge_color='green'

  
plt.figure(figsize=(10,8))
# nx.draw(G,node_color=nod_color,edge_color='green',width=wide,node_size= nod_size,with_labels=True)
# nx.draw_spring(G,node_color=nod_color,edge_color='green',width=wide,node_size= nod_size,with_labels=True)
# 各種不同的繪圖方法 ################################
# input: edges
G = nx.DiGraph()   # directed
# G = nx.Graph()     # undirected    
G.add_weighted_edges_from(edges)
edg =G.edges()
nod=G.nodes()

# 節點顏色大小
nod_size=1
nod_color="yellow"

# 連結寬度
wide=1
edge_color='green'

print (len(df2))        
plt.figure(figsize=(10,8))
nx.draw(G,node_color=nod_color,edge_color='green',width=wide,node_size= nod_size,with_labels=True)
# nx.draw_spring(G,node_color=nod_color,edge_color='green',width=wide,node_size= nod_size,with_labels=True)
# nx.draw_networkx(G,node_color=nod_color,edge_color='green',width=wide,node_size= nod_size,with_labels=True)
#x.draw_spectral(G,node_color=nod_color,edge_color='green',width=wide,node_size= nod_size,with_labels=True)
# nx.draw_random(G,node_color=nod_color,edge_color='orange',width=wide,node_size= nod_size,with_labels=True)
# nx.draw_circular(G,node_color=nod_color,edge_color='green',width=wide,node_size= nod_size,with_labels=True)
plt.show()  

#x.draw_spectral(G,node_color=nod_color,edge_color='green',width=wide,node_size= nod_size,with_labels=True)
# nx.draw_random(G,node_color=nod_color,edge_color='orange',width=wide,node_size= nod_size,with_labels=True)
# nx.draw_circular(G,node_color=nod_color,edge_color='green',width=wide,node_size= nod_size,with_labels=True)
plt.show()

# save excel
path="D:/my python/人民日報/working/"     #先建一個 working 目錄
fname=path+"network李登輝final.xlsx"
writer = pd.ExcelWriter(fname)
df2.to_excel(writer,'Sheet1',encoding='utf-8')
writer.save()

###################################################
import pandas as pd
path="D:/my python/人民日報/working/"    
fname=path+"network李登輝final.xlsx"
df2=pd.read_excel(fname, 0)
df2

"""# nodexl
* https://www.smrfoundation.org/nodexl/
* 填表申請安裝
* 社會網絡分析超好用軟體
* network李登輝final.xlsx, network李登輝.xlsx 均可匯入，然第一個空白欄需先刪除
"""









